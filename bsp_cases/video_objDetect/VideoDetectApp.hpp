#ifndef __VIDEO_DETECT_APP_HPP__
#define __VIDEO_DETECT_APP_HPP__

#include <framework/BasePerfCase.hpp>
#include <shared/BspLogger.hpp>
#include <shared/ArgParser.hpp>
#include <shared/BspFileUtils.hpp>
#include <bsp_dnn/dnnObjDetector.hpp>
#include <bsp_codec/IDecoder.hpp>
#include <bsp_codec/IEncoder.hpp>
#include <bsp_g2d/IGraphics2D.hpp>
#include <memory>
#include <string>
#include <iostream>
#include <vector>
#include <atomic>
#include <mutex>
#include <any>
#include <opencv2/opencv.hpp>

namespace bsp_perf {
namespace perf_cases {
using namespace bsp_dnn;
using namespace bsp_codec;
using namespace bsp_g2d;

class VideoDetectApp : public bsp_perf::common::BasePerfCase
{
public:
    VideoDetectApp(bsp_perf::shared::ArgParser&& args):
        BasePerfCase(std::move(args)),
        m_logger{std::make_unique<bsp_perf::shared::BspLogger>("VideoDetectApp")}
    {
        m_logger->setPattern();
        auto& params = getArgs();
        std::string dnnType;
        std::string pluginPath;
        std::string labelTextPath;
        params.getOptionVal("--dnnType", dnnType);
        params.getOptionVal("--pluginPath", pluginPath);
        params.getOptionVal("--labelTextPath", labelTextPath);
        m_dnnObjDetector = std::make_unique<bsp_dnn::dnnObjDetector>(dnnType, pluginPath, labelTextPath);
    }
    VideoDetectApp(const VideoDetectApp&) = delete;
    VideoDetectApp& operator=(const VideoDetectApp&) = delete;
    VideoDetectApp(VideoDetectApp&&) = delete;
    VideoDetectApp& operator=(VideoDetectApp&&) = delete;
    ~VideoDetectApp() = default;
private:

    void onInit() override
    {
        auto& params = getArgs();
        std::string inputVideoPath;
        params.getOptionVal("--inputVideoPath", inputVideoPath);
        loadVideoFile(inputVideoPath);

        std::string modelPath;
        params.getOptionVal("--modelPath", modelPath);
        m_dnnObjDetector->loadModel(modelPath);

        std::string g2dPlatform;
        params.getOptionVal("--graphics2D", g2dPlatform);
        m_g2d = IGraphics2D::create(g2dPlatform);

        std::string decType;
        params.getOptionVal("--decoderType", decType);
        setupDecoder(decType);

        params.getOptionVal("--encoderType", m_encoderType);

        std::string outputVideoPath;
        params.getOptionVal("--outputVideoPath", outputVideoPath);
        m_out_fp = std::shared_ptr<FILE>(fopen(outputVideoPath.c_str(), "wb"), fclose);

        // ğŸš€ å¯åŠ¨å¼‚æ­¥æ¨ç†çº¿ç¨‹
        m_inference_running = true;
        m_inference_thread = std::thread(&VideoDetectApp::inferenceThreadFunc, this);
        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
            "VideoDetectApp::onInit() Inference thread started");
    }

    void onProcess() override
    {
        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, "read video size: {} bytes", m_videoFileContext->size);
        const int PKT_CHUNK_SIZE = 8192;
        uint8_t* pkt_data_start = m_videoFileContext->data.get();
        uint8_t* pkt_data_end = pkt_data_start + m_videoFileContext->size;

        while (m_videoFileContext->size > 0)
        {
            int pkt_eos = 0;
            int chunk_size = PKT_CHUNK_SIZE;

            if (pkt_data_start + chunk_size >= pkt_data_end)
            {
                pkt_eos = 1;
                chunk_size = pkt_data_end - pkt_data_start;
            }

            DecodePacket dec_pkt = {
                .data = pkt_data_start,
                .pkt_size = chunk_size,
                .pkt_eos = pkt_eos,
            };
            m_decoder->decode(dec_pkt);

            pkt_data_start += chunk_size;

            if (pkt_data_start >= pkt_data_end)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Warn, "VideoDetectApp::onProcess() Video file read complete");
                break;
            }
        }
        // ç­‰å¾…è§£ç å™¨å¤„ç†å®Œæ‰€æœ‰å¸§ï¼ˆè§£ç æ˜¯å¼‚æ­¥çš„ï¼‰
        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
            "VideoDetectApp::onProcess() All data sent to decoder, waiting for decoding to complete...");
        
        // ğŸš€ å…³é”®ï¼šç­‰å¾…è§£ç å®Œæˆå¹¶å…¥é˜Ÿ
        // DNNæ¨ç†æ…¢ä¼šå¯¼è‡´é˜Ÿåˆ—æ»¡ï¼Œdecoder callbacké˜»å¡ï¼Œéœ€è¦ç­‰å¾…è¶³å¤Ÿé•¿çš„æ—¶é—´
        // å¾ªç¯ç­‰å¾…ï¼Œç›´åˆ°é˜Ÿåˆ—ç¨³å®šä¸”æ¨ç†çº¿ç¨‹å¤„ç†é€Ÿåº¦è·Ÿä¸Š
        size_t last_frame_count = 0;
        int stable_count = 0;
        for (int i = 0; i < 60; i++)  // æœ€å¤šç­‰å¾…30ç§’
        {
            std::this_thread::sleep_for(std::chrono::milliseconds(500));
            size_t current_count = m_frame_count.load();
            
            if (current_count == last_frame_count)
            {
                stable_count++;
                if (stable_count >= 3)  // è¿ç»­3æ¬¡ï¼ˆ1.5ç§’ï¼‰æ²¡æœ‰æ–°å¸§ï¼Œè®¤ä¸ºè§£ç å®Œæˆ
                {
                    m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
                        "VideoDetectApp::onProcess() Decoding appears complete, total frames: {}", current_count);
                    break;
                }
            }
            else
            {
                stable_count = 0;
                last_frame_count = current_count;
                if (i % 4 == 0)  // æ¯2ç§’æ‰“å°ä¸€æ¬¡
                {
                    size_t queue_size = 0;
                    {
                        std::lock_guard<std::mutex> lock(m_queue_mutex);
                        queue_size = m_frame_queue.size();
                    }
                    std::cout << "[Main] Processed frames: " << current_count << ", queue size: " << queue_size << std::endl;
                }
            }
        }

        // åœæ­¢æ¨ç†çº¿ç¨‹æ¥æ”¶æ–°å¸§ï¼Œç­‰å¾…å®ƒå¤„ç†å®Œé˜Ÿåˆ—
        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
            "VideoDetectApp::onProcess() Setting stop flag, waiting for inference thread to finish...");
        m_inference_running = false;
        m_queue_cv.notify_all();  // å”¤é†’æ¨ç†çº¿ç¨‹ï¼ˆå¦‚æœåœ¨ç­‰å¾…ï¼‰

        if (m_inference_thread.joinable())
        {
            std::cout << "[Main] Waiting for inference thread to finish, current frames: " << m_frame_count.load() << std::endl;
            m_inference_thread.join();  // ç­‰å¾…æ¨ç†çº¿ç¨‹å¤„ç†å®Œæ‰€æœ‰å¸§å¹¶å‘é€EOS
            std::cout << "[Main] Inference thread joined, all frames processed: " << m_frame_count.load() << std::endl;
        }

        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
            "VideoDetectApp::onProcess() All frames processed and encoded, total: {}", m_frame_count.load());
    }

    void onRender() override
    {
        ;
    }

    void onRelease() override
    {
        // æ¨ç†çº¿ç¨‹å·²ç»åœ¨ onProcess() ä¸­joinäº†ï¼Œè¿™é‡Œåªéœ€è¦æ¸…ç†èµ„æº
        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
            "VideoDetectApp::onRelease() Starting cleanup, inference thread already joined");

        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
            "VideoDetectApp::onRelease() Waiting for encoding to finish...");
        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
            "Total frames submitted: {}, encoded: {}", m_frame_count.load(), m_encoded_frame_count.load());
        // ç­‰å¾…æ‰€æœ‰å¸§ç¼–ç å®Œæˆ
        // å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿ç¼–ç å™¨æœ‰è¶³å¤Ÿæ—¶é—´å¤„ç† EOS å’Œåˆ·æ–°ç¼“å†²åŒº
        for (int i = 0; i < 10; i++)
        {
            std::this_thread::sleep_for(std::chrono::milliseconds(500));
            if (m_encoded_frame_count >= m_frame_count)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
                    "All frames encoded: {}/{}", m_encoded_frame_count.load(), m_frame_count.load());
                break;
            }
            if (i % 2 == 0)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Debug, 
                    "Waiting... encoded: {}/{}", m_encoded_frame_count.load(), m_frame_count.load());
            }
        }
        if (m_out_fp)
        {
            fflush(m_out_fp.get());
            m_out_fp.reset();
            m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
                "VideoDetectApp::onRelease() Output video file closed");
        }

        // å…ˆé”€æ¯ç¼–ç å™¨ï¼Œå†é”€æ¯å…¶ä»–èµ„æº
        m_encoder.reset();
        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
            "VideoDetectApp::onRelease() Encoder released");

        m_dnnObjDetector.reset();
        BspFileUtils::ReleaseFileMmap(m_videoFileContext);

        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info,
            "VideoDetectApp::onRelease() Resources released, final encoded frames: {}", 
            m_encoded_frame_count.load());
    }

    void onEncodeReady(std::any userdata, const char* data, int size)
    {
        std::cout << "[onEncodeReady] CALLED with size=" << size << ", m_encoded_frame_count=" << m_encoded_frame_count.load() << std::endl;
        if (data && size > 0 && m_out_fp)
        {
            std::lock_guard<std::mutex> lock(m_file_mutex);
            size_t written = fwrite(data, 1, size, m_out_fp.get());

            if (written != (size_t)size)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error,
                    "VideoDetectApp::onEncodeReady() Write failed, expected: {}, written: {}", size, written);
                return;
            }

            m_encoded_frame_count++;
            std::cout << "zczjx--> onEncodeReady, m_encoded_frame_count: " << m_encoded_frame_count.load() << " (submitted: " << m_frame_count.load() << ")" << std::endl;
            // Log progress every 30 frames
            if (m_encoded_frame_count % 30 == 0)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info,
                    "VideoDetectApp::onEncodeReady() Encoded frame {}, size: {} bytes",
                    m_encoded_frame_count.load(), size);
            }
        }
        else
        {
            std::cout << "[onEncodeReady] WARNING: Invalid data or file pointer!" << std::endl;
        }
    }

private:
    void loadVideoFile(std::string& videoPath)
    {
        m_decode_format = BspFileUtils::getFileExtension(videoPath);

        if(m_decode_format.compare("h264") == 0 || m_decode_format.compare("h265") == 0)
        {
            m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, "VideoDetectApp::format: {}", m_decode_format);
        }
        else
        {
            m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error, "VideoDetectApp::onInit() Unsupported video format: {}", m_decode_format);
            throw std::runtime_error("Unsupported video format.");
        }
        m_videoFileContext = BspFileUtils::LoadFileMmap(videoPath);
    }

    void setupDecoder(std::string& decType)
    {
        m_decoder = IDecoder::create(decType);
        DecodeConfig cfg = {
            .encoding = m_decode_format,
            .fps = 30,
        };
        m_decoder->setup(cfg);

        auto decoderCallback = [this](std::any /*userdata*/, std::shared_ptr<DecodeOutFrame> frame)
        {
            if (m_encoder == nullptr)
            {
                m_encoder = IEncoder::create(m_encoderType);

                // ä¿®å¤ stride ä¸º 0 çš„é—®é¢˜ï¼šå½“ stride ä¸º 0 æ—¶ï¼Œä½¿ç”¨ width/height ä½œä¸ºé»˜è®¤å€¼
                int hor_stride = (frame->width_stride > 0) ? frame->width_stride : frame->width;
                int ver_stride = (frame->height_stride > 0) ? frame->height_stride : frame->height;
                EncodeConfig enc_cfg = {
                    .encodingType = "h264",
                    .frameFormat = "YUV420SP",
                    .fps = 30,
                    .width = frame->width,
                    .height = frame->height,
                    .hor_stride = hor_stride,
                    .ver_stride = ver_stride,
                };
                int ret = m_encoder->setup(enc_cfg);
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
                    "VideoDetectApp encoder setup ret: {}, width: {}, height: {}, hor_stride: {}, ver_stride: {}", 
                    ret, frame->width, frame->height, hor_stride, ver_stride);
                // è®¾ç½®ç¼–ç å®Œæˆå›è°ƒï¼ˆå‚è€ƒ EncodeApp.hppï¼‰
                m_encoder->setEncodeReadyCallback(
                    [this](std::any userdata, const char* data, int size) {
                        this->onEncodeReady(userdata, data, size);
                    },
                    std::any(this)
                );

                // è·å–å¹¶å†™å…¥ç¼–ç å™¨å¤´éƒ¨ï¼ˆå¦‚ SPS/PPSï¼‰
                std::string enc_header;
                m_encoder->getEncoderHeader(enc_header);
                if (!enc_header.empty())
                {
                    m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
                        "VideoDetectApp::onProcess() Write encoder header enc_header.size(): {}", enc_header.size());
                    fwrite(enc_header.c_str(), 1, enc_header.size(), m_out_fp.get());
                    fflush(m_out_fp.get());
                }
            }
            m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Debug, "frame->width: {}, frame->height: {}, frame->width_stride: {}, frame->height_stride: {}, frame->format: {}, frame->valid_data_size: {}", frame->width, frame->height, frame->width_stride, frame->height_stride, frame->format, frame->valid_data_size);

            // âš ï¸ ç¡®ä¿ç¼–ç å™¨åœ¨å…¥é˜Ÿå‰å°±ç»ªï¼ˆé¿å…æ­»é”ï¼‰
            // æ¨ç†çº¿ç¨‹éœ€è¦ç¼–ç å™¨ï¼Œå¦‚æœç¼–ç å™¨åœ¨å›è°ƒä¸­åˆ›å»ºä½†å›è°ƒè¢«é˜Ÿåˆ—æ»¡é˜»å¡ï¼Œå°±ä¼šæ­»é”
            if (m_encoder == nullptr)
            {
                std::cout << "[Decoder] WARNING: Encoder not ready yet, skipping frame for now" << std::endl;
                return;  // è·³è¿‡è¿™ä¸€å¸§ï¼Œç­‰ä¸‹ä¸€å¸§å†åˆ›å»ºç¼–ç å™¨å’Œå…¥é˜Ÿ
            }

            // ğŸš€ å¼‚æ­¥æ¶æ„ï¼šå¿«é€Ÿå…¥é˜Ÿï¼Œä¸é˜»å¡è§£ç å™¨
            // æ‹·è´å¸§æ•°æ®ï¼ˆé‡è¦ï¼é¿å… decoder buffer è¢«ä¸‹ä¸€å¸§è¦†ç›–ï¼‰
            FrameTask task;
            task.frame_data.resize(frame->valid_data_size);
            std::memcpy(task.frame_data.data(), frame->virt_addr, frame->valid_data_size);
            task.width = frame->width;
            task.height = frame->height;
            task.format = frame->format;
            task.eos_flag = frame->eos_flag;

            {
                std::unique_lock<std::mutex> lock(m_queue_mutex);
                // ğŸ”§ å…³é”®ä¿®å¤ï¼šæ€»æ˜¯ç­‰å¾…é˜Ÿåˆ—æœ‰ç©ºé—´ï¼Œä¸è¦å› ä¸ºåœæ­¢æ ‡å¿—å°±è·³è¿‡ç­‰å¾…
                // è¿™æ ·å¯ä»¥ç¡®ä¿æ‰€æœ‰è§£ç çš„å¸§éƒ½è¢«å…¥é˜Ÿï¼Œä¸ä¼šå› ä¸ºæ¨ç†æ…¢è€Œä¸¢å¸§
                m_queue_cv.wait(lock, [this] { return m_frame_queue.size() < MAX_QUEUE_SIZE; });

                // ç›´æ¥å…¥é˜Ÿï¼Œä¸æ£€æŸ¥ m_inference_running
                m_frame_queue.push(std::move(task));

                // å®šæœŸæ‰“å°é˜Ÿåˆ—çŠ¶æ€
                if (m_frame_queue.size() % 50 == 0)
                {
                    std::cout << "[Decoder] Frame queued, queue size: " << m_frame_queue.size() << std::endl;
                }
            }
            m_queue_cv.notify_one();  // å”¤é†’æ¨ç†çº¿ç¨‹
            // âœ… è§£ç å™¨å›è°ƒç«‹å³è¿”å›ï¼Œä¸é˜»å¡ï¼
            // æ¨ç†å’Œç¼–ç åœ¨ç‹¬ç«‹çš„æ¨ç†çº¿ç¨‹ä¸­å¼‚æ­¥æ‰§è¡Œ
        };

        m_decoder->setDecodeReadyCallback(decoderCallback, nullptr);
    }

    void setObjDetectParams(ObjDetectParams& objDetectParams, std::shared_ptr<DecodeOutFrame> frame)
    {
        IDnnEngine::dnnInputShape shape;
        m_dnnObjDetector->getInputShape(shape);
        auto& params = getArgs();
        objDetectParams.model_input_width = shape.width;
        objDetectParams.model_input_height = shape.height;
        objDetectParams.model_input_channel = shape.channel;
        params.getSubOptionVal("objDetectParams", "--conf_threshold", objDetectParams.conf_threshold);
        params.getSubOptionVal("objDetectParams", "--nms_threshold", objDetectParams.nms_threshold);
        objDetectParams.scale_width = static_cast<float>(shape.width) / static_cast<float>(frame->width);
        objDetectParams.scale_height = static_cast<float>(shape.height) / static_cast<float>(frame->height);
        params.getSubOptionVal("objDetectParams", "--pads_left", objDetectParams.pads.left);
        params.getSubOptionVal("objDetectParams", "--pads_right", objDetectParams.pads.right);
        params.getSubOptionVal("objDetectParams", "--pads_top", objDetectParams.pads.top);
        params.getSubOptionVal("objDetectParams", "--pads_bottom", objDetectParams.pads.bottom);
        m_dnnObjDetector->getOutputQuantParams(objDetectParams.quantize_zero_points, objDetectParams.quantize_scales);
    }

    std::vector<ObjDetectOutputBox> dnnInference(std::shared_ptr<DecodeOutFrame> frame)
    {
        bsp_dnn::ObjDetectInput objDetectInput = {
            .handleType = "DecodeOutFrame",
            .imageHandle = frame,
        };
        m_dnnObjDetector->pushInputData(std::make_shared<bsp_dnn::ObjDetectInput>(objDetectInput));
        setObjDetectParams(m_objDetectParams, frame);
        m_dnnObjDetector->runObjDetect(m_objDetectParams);
        return m_dnnObjDetector->popOutputData();
    }

private:
    std::string m_name {"[VideoDetectApp]:"};
    std::unique_ptr<bsp_perf::shared::BspLogger> m_logger{nullptr};
    std::unique_ptr<bsp_dnn::dnnObjDetector> m_dnnObjDetector{nullptr};
    bsp_dnn::ObjDetectParams m_objDetectParams{};
    std::shared_ptr<BspFileUtils::FileContext> m_videoFileContext{nullptr};
    std::vector<uint8_t> m_rgba_buf{};  // RGB888 buffer for NvVic conversion and drawing bbox (èŠ‚çœå†…å­˜ï¼Œ3å­—èŠ‚/åƒç´ )
    std::vector<uint8_t> m_yuv420_buf{};  // YUV420 buffer for encoder input

    std::string m_encoderType{""};
    std::unique_ptr<IDecoder> m_decoder{nullptr};
    std::unique_ptr<IEncoder> m_encoder{nullptr};
    std::unique_ptr<IGraphics2D> m_g2d{nullptr};
    std::string m_decode_format{""};
    std::shared_ptr<FILE> m_out_fp{nullptr};

    // å¸§è®¡æ•°å™¨
    std::atomic<size_t> m_frame_count{0};          // æäº¤åˆ°ç¼–ç å™¨çš„å¸§æ•°
    std::atomic<size_t> m_encoded_frame_count{0};  // ç¼–ç å®Œæˆçš„å¸§æ•°
    std::mutex m_file_mutex;                        // ä¿æŠ¤æ–‡ä»¶å†™å…¥çš„äº’æ–¥é”

    std::map<std::string, cv::Scalar> m_labelColorMap;
    std::vector<cv::Scalar> m_colors_list = {
        cv::Scalar(128, 128, 0),  // Teal
        cv::Scalar(0, 128, 128),   // Aqua
        cv::Scalar(255, 0, 0),    // Blue
        cv::Scalar(0, 0, 255),    // Red
        cv::Scalar(0, 255, 0),    // Green
        cv::Scalar(255, 255, 0),  // Cyan
        cv::Scalar(128, 128, 128), // Gray
        cv::Scalar(192, 192, 192), // Silver
        cv::Scalar(255, 165, 0),   // Orange
        cv::Scalar(255, 20, 147),  // DeepPink
        cv::Scalar(75, 0, 130),    // Indigo
        cv::Scalar(240, 230, 140), // Khaki
        cv::Scalar(255, 0, 255),  // Magenta
        cv::Scalar(128, 0, 128),  // Purple
        cv::Scalar(0, 255, 255),  // Yellow
        cv::Scalar(128, 0, 0),    // Maroon
        cv::Scalar(0, 128, 0),    // Olive
        cv::Scalar(0, 0, 128),    // Navy
        cv::Scalar(173, 216, 230), // LightBlue
        cv::Scalar(255, 182, 193), // LightPink
        cv::Scalar(144, 238, 144), // LightGreen
        cv::Scalar(255, 255, 224)  // LightYellow
    };

    // ğŸ”§ å¼‚æ­¥æ¨ç†æ¶æ„ï¼šè§£ç  â†’ é˜Ÿåˆ— â†’ æ¨ç†çº¿ç¨‹ â†’ ç¼–ç 
    // è§£å†³ DNN inference é˜»å¡è§£ç å™¨çš„é—®é¢˜
    struct FrameTask {
        std::vector<uint8_t> frame_data;  // æ‹·è´å¸§æ•°æ®ï¼ˆé¿å… decoder buffer è¢«è¦†ç›–ï¼‰
        uint32_t width;
        uint32_t height;
        std::string format;
        bool eos_flag;
    };
    std::queue<FrameTask> m_frame_queue;
    std::mutex m_queue_mutex;
    std::condition_variable m_queue_cv;
    std::thread m_inference_thread;
    std::atomic<bool> m_inference_running{false};
    static constexpr size_t MAX_QUEUE_SIZE = 30;  // æœ€å¤§é˜Ÿåˆ—é•¿åº¦

    // æ¨ç†çº¿ç¨‹å‡½æ•°
    void inferenceThreadFunc()
    {
        std::cout << "[Inference Thread] Starting, m_inference_running=" << m_inference_running << std::endl;
        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
            "VideoDetectApp::inferenceThreadFunc() Inference thread started");

        int loop_count = 0;
        // ğŸ”§ å…³é”®ä¿®å¤ï¼šç»§ç»­å¤„ç†ç›´åˆ°é˜Ÿåˆ—ä¸ºç©ºï¼ˆä¸è¦å› ä¸º m_inference_running=false å°±é€€å‡ºï¼‰
        while (true)
        {
            loop_count++;
            if (loop_count % 10 == 1) {
                std::cout << "[Inference Thread] Loop " << loop_count << ", waiting for frames..." << std::endl;
            }
            
            FrameTask task;
            bool has_task = false;
            {
                std::unique_lock<std::mutex> lock(m_queue_mutex);
                std::cout << "[Inference Thread] Before wait, queue size=" << m_frame_queue.size() 
                          << ", m_inference_running=" << m_inference_running << std::endl;
                
                // ç­‰å¾…é˜Ÿåˆ—ä¸­æœ‰ä»»åŠ¡
                m_queue_cv.wait(lock, [this] { 
                    bool should_wake = !m_frame_queue.empty() || !m_inference_running;
                    return should_wake;
                });
                
                std::cout << "[Inference Thread] After wait, queue size=" << m_frame_queue.size() 
                          << ", m_inference_running=" << m_inference_running << std::endl;
                
                if (!m_frame_queue.empty())
                {
                    task = std::move(m_frame_queue.front());
                    m_frame_queue.pop();
                    has_task = true;
                    std::cout << "[Inference Thread] Got frame from queue, new size=" << m_frame_queue.size() << std::endl;
                }
                else if (!m_inference_running)
                {
                    // é˜Ÿåˆ—ä¸ºç©ºä¸”æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡ºçº¿ç¨‹
                    std::cout << "[Inference Thread] Exiting: queue empty and m_inference_running=false" << std::endl;
                    break;
                }
                else
                {
                    // å‡å”¤é†’ï¼Œç»§ç»­ç­‰å¾…
                    std::cout << "[Inference Thread] Spurious wakeup, continuing..." << std::endl;
                    continue;
                }
            }
            m_queue_cv.notify_all();  // é€šçŸ¥å¯èƒ½ç­‰å¾…çš„ decoderï¼ˆé˜Ÿåˆ—æœ‰ç©ºé—´äº†ï¼‰

            if (!has_task) {
                continue;  // åº”è¯¥ä¸ä¼šåˆ°è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨
            }

            // âš ï¸ ç­‰å¾…ç¼–ç å™¨å°±ç»ªï¼ˆç¬¬ä¸€å¸§ä¼šåˆ›å»ºç¼–ç å™¨ï¼‰
            int wait_count = 0;
            while (m_encoder == nullptr)
            {
                wait_count++;
                if (wait_count == 1 || wait_count % 10 == 0) {
                    std::cout << "[Inference Thread] Waiting for encoder (attempt " << wait_count << ")..." << std::endl;
                }
                std::this_thread::sleep_for(std::chrono::milliseconds(10));

                // å¦‚æœç­‰å¾…è¶…è¿‡1ç§’ä¸”æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ”¾å¼ƒè¿™ä¸€å¸§
                if (wait_count > 100 && !m_inference_running) {
                    std::cout << "[Inference Thread] Encoder not ready after 1s, giving up on this frame" << std::endl;
                    break;
                }
            }

            if (!m_encoder)
            {
                std::cout << "[Inference Thread] Encoder not ready, skipping frame" << std::endl;
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Warn,
                    "VideoDetectApp::inferenceThreadFunc() Encoder not ready, skipping frame");
                continue;
            }

            // æ‰§è¡Œ DNN æ¨ç†ï¼ˆåœ¨æ¨ç†çº¿ç¨‹ä¸­ï¼Œä¸é˜»å¡è§£ç å™¨ï¼‰
            auto frame = std::make_shared<DecodeOutFrame>();
            frame->virt_addr = task.frame_data.data();
            frame->width = task.width;
            frame->height = task.height;
            frame->format = task.format;
            frame->valid_data_size = task.frame_data.size();
            frame->eos_flag = task.eos_flag;

            // æ‰§è¡Œ DNN æ¨ç†ï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡è§£ç å™¨ï¼‰
            auto objDetectOutput = dnnInference(frame);

            // ğŸ” è°ƒè¯•ï¼šæ‰“å°æ£€æµ‹ç»“æœæ•°é‡
            std::cout << "[Inference Thread] Frame " << m_frame_count.load() 
                      << " detected " << objDetectOutput.size() << " objects" << std::endl;
            for (const auto& item : objDetectOutput)
            {
                std::cout << "  - " << item.label << " @ (" 
                          << item.bbox.left << "," << item.bbox.top << ")-(" 
                          << item.bbox.right << "," << item.bbox.bottom 
                          << ") score=" << item.score << std::endl;
            }

            // ä¿®å¤ stride ä¸º 0 çš„é—®é¢˜
            size_t input_width_stride = frame->width_stride > 0 ? 
                                        static_cast<size_t>(frame->width_stride) : 
                                        static_cast<size_t>(frame->width);
            size_t input_height_stride = frame->height_stride > 0 ? 
                                         static_cast<size_t>(frame->height_stride) : 
                                         static_cast<size_t>(frame->height);

            // âš ï¸ å…³é”®ä¿®å¤ï¼šYUV â†’ RGBA ä¹Ÿéœ€è¦ä½¿ç”¨ handle + imageCopy æ¨¡å¼
            // æ­¥éª¤1: åˆ›å»ºè¾“å…¥ YUV420 å¸§çš„ G2DBufferï¼ˆvirtualaddr æ¨¡å¼ï¼‰
            IGraphics2D::G2DBufferParams dec_out_g2d_params = {
                .virtual_addr = task.frame_data.data(),
                .rawBufferSize = task.frame_data.size(),  // âš ï¸ å…³é”®ï¼šå¿…é¡»è®¾ç½®ï¼
                .width = static_cast<size_t>(task.width),
                .height = static_cast<size_t>(task.height),
                .width_stride = input_width_stride,
                .height_stride = input_height_stride,
                .format = task.format,
            };
            std::shared_ptr<IGraphics2D::G2DBuffer> g2d_dec_out_buf = m_g2d->createG2DBuffer("virtualaddr", dec_out_g2d_params);

            // æ­¥éª¤2: åˆ›å»º RGBA8888 handle bufferï¼ˆè®© NvVic ç®¡ç†ï¼Œç”¨äºæ¥æ”¶è½¬æ¢ç»“æœï¼‰
            size_t rgba_buffer_size = frame->width * frame->height * 4;  // RGBA8888: 4 å­—èŠ‚/åƒç´ 
            if (m_rgba_buf.size() != rgba_buffer_size)
            {
                m_rgba_buf.resize(rgba_buffer_size);
            }

            IGraphics2D::G2DBufferParams rgba_handle_params = {
                .virtual_addr = nullptr,  // âš ï¸ handle æ¨¡å¼ï¼Œå¿…é¡»æ˜¯ nullptr
                .rawBufferSize = rgba_buffer_size,
                .width = static_cast<size_t>(frame->width),
                .height = static_cast<size_t>(frame->height),
                .width_stride = static_cast<size_t>(frame->width),
                .height_stride = static_cast<size_t>(frame->height),
                .format = "RGBA8888",
            };
            std::shared_ptr<IGraphics2D::G2DBuffer> rgba_handle_buf = m_g2d->createG2DBuffer("handle", rgba_handle_params);

            // æ­¥éª¤3: æ‰§è¡Œ YUV420 åˆ° RGBA8888 çš„è½¬æ¢ï¼ˆè½¬æ¢åˆ° handle bufferï¼‰
            int ret = m_g2d->imageCvtColor(g2d_dec_out_buf, rgba_handle_buf, task.format, "RGBA8888");
            if (ret != 0)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error,
                    "VideoDetectApp::inferenceThreadFunc() YUV to RGBA conversion failed: {}", ret);
                m_g2d->releaseG2DBuffer(rgba_handle_buf);
                m_g2d->releaseG2DBuffer(g2d_dec_out_buf);
                continue;
            }

            std::cout << "[Inference Thread] YUV to RGBA conversion succeeded" << std::endl;

            // æ­¥éª¤4: ä» handle buffer å¤åˆ¶åˆ° host bufferï¼ˆä¾› OpenCV ä½¿ç”¨ï¼‰
            IGraphics2D::G2DBufferParams rgba_host_params = {
                .virtual_addr = m_rgba_buf.data(),
                .rawBufferSize = rgba_buffer_size,
                .width = static_cast<size_t>(task.width),
                .height = static_cast<size_t>(task.height),
                .width_stride = static_cast<size_t>(task.width),
                .height_stride = static_cast<size_t>(task.height),
                .format = "RGBA8888",
            };
            std::shared_ptr<IGraphics2D::G2DBuffer> rgba_host_buf = m_g2d->createG2DBuffer("virtualaddr", rgba_host_params);
            if (!rgba_host_buf)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error,
                    "VideoDetectApp::inferenceThreadFunc() Failed to create RGBA host buffer");
                m_g2d->releaseG2DBuffer(rgba_handle_buf);
                m_g2d->releaseG2DBuffer(g2d_dec_out_buf);
                continue;
            }
            
            ret = m_g2d->imageCopy(rgba_handle_buf, rgba_host_buf);
            if (ret != 0)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error,
                    "VideoDetectApp::inferenceThreadFunc() RGBA copy to host failed: {}", ret);
                m_g2d->releaseG2DBuffer(rgba_host_buf);
                m_g2d->releaseG2DBuffer(rgba_handle_buf);
                m_g2d->releaseG2DBuffer(g2d_dec_out_buf);
                continue;
            }

            std::cout << "[Inference Thread] RGBA copied to host buffer for drawing" << std::endl;

            // æ­¥éª¤5: ä½¿ç”¨ OpenCV åœ¨ RGBA8888 å›¾åƒä¸Šç”» bbox
            cv::Mat cvRGBAImage(task.height, task.width, CV_8UC4, m_rgba_buf.data());

            int i = 1 + (std::rand() % m_colors_list.size());

            for (const auto& item : objDetectOutput)
            {
                if (m_labelColorMap.find(item.label) == m_labelColorMap.end())
                {
                    m_labelColorMap[item.label] = m_colors_list[i % m_colors_list.size()];
                    i++;
                }
                cv::rectangle(cvRGBAImage, cv::Point(item.bbox.left, item.bbox.top), cv::Point(item.bbox.right, item.bbox.bottom),
                    m_labelColorMap[item.label], 2);
                cv::putText(cvRGBAImage, item.label, cv::Point(item.bbox.left, item.bbox.top + 12), cv::FONT_HERSHEY_COMPLEX, 0.4, cv::Scalar(255, 255, 255, 255));
            }

            std::cout << "[Inference Thread] Drew " << objDetectOutput.size() << " detection boxes" << std::endl;

            // æ­¥éª¤6: å°†ç”»å¥½æ¡†çš„ RGBA æ•°æ®å¤åˆ¶å› handle buffer
            // âš ï¸ å…³é”®ä¿®å¤ï¼šå¿…é¡»é‡æ–°åˆ›å»º rgba_host_bufï¼Œè®© createG2DBuffer å°†ä¿®æ”¹åçš„ m_rgba_buf å¤åˆ¶åˆ° NvBufSurface
            // å› ä¸º OpenCV ä¿®æ”¹çš„æ˜¯ m_rgba_buf å†…å­˜ï¼Œè€Œ imageCopy éœ€è¦ä» NvBufSurface è¯»å–
            // å…ˆé‡Šæ”¾æ—§çš„ rgba_host_buf
            m_g2d->releaseG2DBuffer(rgba_host_buf);
            
            // é‡æ–°åˆ›å»º rgba_host_bufï¼ŒcreateG2DBuffer ä¼šè‡ªåŠ¨å°†ç”»å¥½æ¡†çš„ m_rgba_buf å¤åˆ¶åˆ° NvBufSurface
            rgba_host_buf = m_g2d->createG2DBuffer("virtualaddr", rgba_host_params);
            if (!rgba_host_buf)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error,
                    "VideoDetectApp::inferenceThreadFunc() Failed to recreate RGBA host buffer after drawing");
                m_g2d->releaseG2DBuffer(rgba_handle_buf);
                m_g2d->releaseG2DBuffer(g2d_dec_out_buf);
                continue;
            }
            
            // ç°åœ¨å¤åˆ¶å› handle bufferï¼ˆæ­¤æ—¶ NvBufSurface ä¸­æœ‰ç”»å¥½æ¡†çš„æ•°æ®ï¼‰
            ret = m_g2d->imageCopy(rgba_host_buf, rgba_handle_buf);
            if (ret != 0)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error,
                    "VideoDetectApp::inferenceThreadFunc() RGBA copy back to handle failed: {}", ret);
                m_g2d->releaseG2DBuffer(rgba_host_buf);
                m_g2d->releaseG2DBuffer(rgba_handle_buf);
                m_g2d->releaseG2DBuffer(g2d_dec_out_buf);
                continue;
            }
            
            std::cout << "[Inference Thread] RGBA copied back to handle buffer" << std::endl;

            // âš ï¸ ç«‹å³é‡Šæ”¾ rgba_host_bufï¼Œå‡å°‘èµ„æºå ç”¨
            m_g2d->releaseG2DBuffer(rgba_host_buf);

            // æ­¥éª¤6: åˆ›å»º YUV420 handle bufferï¼ˆè®© NvVic ç®¡ç†ï¼Œç”¨äºæ¥æ”¶è½¬æ¢ç»“æœï¼‰
            size_t yuv420_buffer_size = task.width * task.height * 3 / 2;
            if (m_yuv420_buf.size() != yuv420_buffer_size)
            {
                m_yuv420_buf.resize(yuv420_buffer_size);
            }

            IGraphics2D::G2DBufferParams yuv420_handle_params = {
                .virtual_addr = nullptr,  // âš ï¸ handle æ¨¡å¼ï¼Œå¿…é¡»æ˜¯ nullptr
                .rawBufferSize = yuv420_buffer_size,
                .width = static_cast<size_t>(task.width),
                .height = static_cast<size_t>(task.height),
                .width_stride = input_width_stride,
                .height_stride = input_height_stride,
                .format = task.format,
            };
            std::shared_ptr<IGraphics2D::G2DBuffer> yuv420_handle_buf = m_g2d->createG2DBuffer("handle", yuv420_handle_params);

            // æ­¥éª¤7: å°†ç”»å¥½æ¡†çš„ RGBA8888 è½¬æ¢å› YUV420ï¼ˆä» handle buffer è½¬æ¢åˆ° handle bufferï¼‰
            ret = m_g2d->imageCvtColor(rgba_handle_buf, yuv420_handle_buf, "RGBA8888", task.format);
            if (ret != 0)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error,
                    "VideoDetectApp::inferenceThreadFunc() RGBA to YUV conversion failed: {}", ret);
                m_g2d->releaseG2DBuffer(yuv420_handle_buf);
                m_g2d->releaseG2DBuffer(rgba_handle_buf);
                m_g2d->releaseG2DBuffer(g2d_dec_out_buf);
                continue;
            }

            std::cout << "[Inference Thread] RGBA to YUV conversion succeeded" << std::endl;

            // æ­¥éª¤8: âš ï¸ å…³é”®ï¼å°† YUV æ•°æ®ä» handle buffer å¤åˆ¶åˆ°ç”¨æˆ· bufferï¼ˆä¾›ç¼–ç å™¨ä½¿ç”¨ï¼‰
            IGraphics2D::G2DBufferParams yuv420_host_params = {
                .virtual_addr = m_yuv420_buf.data(),
                .rawBufferSize = yuv420_buffer_size,
                .width = static_cast<size_t>(task.width),
                .height = static_cast<size_t>(task.height),
                .width_stride = input_width_stride,
                .height_stride = input_height_stride,
                .format = task.format,
            };
            std::shared_ptr<IGraphics2D::G2DBuffer> yuv420_host_buf = m_g2d->createG2DBuffer("virtualaddr", yuv420_host_params);
            if (!yuv420_host_buf)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error,
                    "VideoDetectApp::inferenceThreadFunc() Failed to create YUV host buffer");
                m_g2d->releaseG2DBuffer(yuv420_handle_buf);
                m_g2d->releaseG2DBuffer(rgba_handle_buf);
                m_g2d->releaseG2DBuffer(g2d_dec_out_buf);
                continue;
            }

            ret = m_g2d->imageCopy(yuv420_handle_buf, yuv420_host_buf);
            if (ret != 0)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error,
                    "VideoDetectApp::inferenceThreadFunc() YUV copy to host failed: {}", ret);
                m_g2d->releaseG2DBuffer(yuv420_host_buf);
                m_g2d->releaseG2DBuffer(yuv420_handle_buf);
                m_g2d->releaseG2DBuffer(rgba_handle_buf);
                m_g2d->releaseG2DBuffer(g2d_dec_out_buf);
                continue;
            }

            std::cout << "[Inference Thread] YUV copied to host buffer for encoder" << std::endl;

            // æ­¥éª¤9: é‡Šæ”¾æ‰€æœ‰ G2D buffersï¼ˆé¿å…èµ„æºæ³„æ¼ï¼‰
            // âš ï¸ å…³é”®ä¼˜åŒ–ï¼šæŒ‰åˆ›å»ºçš„é€†åºé‡Šæ”¾ï¼Œå¹¶ç«‹å³é‡Šæ”¾ä¸å†ä½¿ç”¨çš„ buffer
            m_g2d->releaseG2DBuffer(yuv420_host_buf);
            m_g2d->releaseG2DBuffer(yuv420_handle_buf);
            m_g2d->releaseG2DBuffer(rgba_handle_buf);
            m_g2d->releaseG2DBuffer(g2d_dec_out_buf);

            // è·å–ç¼–ç å™¨è¾“å…¥ç¼“å†²åŒº
            std::shared_ptr<EncodeInputBuffer> enc_in_buf = m_encoder->getInputBuffer();
            if (!enc_in_buf)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error,
                    "VideoDetectApp::inferenceThreadFunc() Failed to get input buffer");
                continue;
            }

            // å°†ç”»å¥½ bbox çš„ YUV420 æ•°æ®ä¼ ç»™ç¼–ç å™¨è¿›è¡Œå‹ç¼©ç¼–ç 
            enc_in_buf->input_buf_addr = m_yuv420_buf.data();

            // å‡†å¤‡ç¼–ç è¾“å‡ºåŒ…
            EncodePacket enc_pkt = {
                .max_size = m_encoder->getFrameSize() * 2,
                .pkt_eos = task.eos_flag,
                .pkt_len = 0,
            };
            enc_pkt.encode_pkt.resize(enc_pkt.max_size);

            // æ‰§è¡Œç¼–ç 
            int encode_ret = m_encoder->encode(*enc_in_buf, enc_pkt);
            if (encode_ret != 0)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error, 
                    "VideoDetectApp::inferenceThreadFunc() encode failed, ret: {}", encode_ret);
                continue;
            }

            m_frame_count++;
            if (m_frame_count % 30 == 0)
            {
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
                    "VideoDetectApp::inferenceThreadFunc() Processed {} frames", m_frame_count.load());
            }
        }

        // ğŸ›‘ æ‰€æœ‰å¸§å¤„ç†å®Œæ¯•ï¼Œå‘é€EOSç»™ç¼–ç å™¨
        std::cout << "[Inference Thread] All frames processed, sending EOS to encoder" << std::endl;
        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
            "VideoDetectApp::inferenceThreadFunc() All frames processed ({}), sending EOS to encoder", 
            m_frame_count.load());

        if (m_encoder)
        {
            std::shared_ptr<EncodeInputBuffer> inputBuf = m_encoder->getInputBuffer();
            if (inputBuf)
            {
                EncodePacket eosPkt;
                eosPkt.max_size = m_encoder->getFrameSize() * 2;
                eosPkt.pkt_eos = 1;  // EOSæ ‡å¿—
                eosPkt.pkt_len = 0;
                eosPkt.encode_pkt.resize(eosPkt.max_size);
                int ret = m_encoder->encode(*inputBuf, eosPkt);
                std::cout << "[Inference Thread] EOS sent to encoder, ret=" << ret << std::endl;
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
                    "VideoDetectApp::inferenceThreadFunc() EOS sent to encoder, ret: {}", ret);
            }
            else
            {
                std::cout << "[Inference Thread] ERROR: Failed to get input buffer for EOS" << std::endl;
                m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Error, 
                    "VideoDetectApp::inferenceThreadFunc() Failed to get input buffer for EOS");
            }
        }

        m_logger->printStdoutLog(bsp_perf::shared::BspLogger::LogLevel::Info, 
            "VideoDetectApp::inferenceThreadFunc() Inference thread exiting, processed {} frames", m_frame_count.load());
    }
};


} // namespace perf_cases
} // namespace bsp_perf

#endif // __VIDEO_DETECT_APP_HPP__